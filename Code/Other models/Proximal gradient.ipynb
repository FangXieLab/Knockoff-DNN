{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# One layer and Multiple layers filters","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom scipy.sparse.linalg import eigsh\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-06-26T15:15:18.838561Z","iopub.execute_input":"2024-06-26T15:15:18.839029Z","iopub.status.idle":"2024-06-26T15:15:34.777189Z","shell.execute_reply.started":"2024-06-26T15:15:18.838990Z","shell.execute_reply":"2024-06-26T15:15:34.775673Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-26 15:15:22.184639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 15:15:22.184791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 15:15:22.341597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Creat simulation variables","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef initialize_parameters(input_size, hidden_size, output_size):\n    np.random.seed(42)\n    W1 = np.random.normal(loc=0, scale=1, size=(input_size, hidden_size))\n    b1 = np.zeros((1, hidden_size))\n    W2 = np.random.normal(loc=0, scale=1, size=(hidden_size, output_size))\n    b2 = np.zeros((1, output_size))\n    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\ndef forward_propagation(X, parameters):\n    Z1 = np.dot(X, parameters['W1']) + parameters['b1']\n    A1 = relu(Z1)\n    Z2 = np.dot(A1, parameters['W2']) + parameters['b2']\n    A2 = sigmoid(Z2)\n    return {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n\ndef predict(X, parameters):\n    forward = forward_propagation(X, parameters)\n    return (forward['A2'] > 0.5).astype(int)\n\n# Set up neural network parameters\ninput_size = 100\nhidden_size = 264\noutput_size = 1\nn_samples = 1000\n\n# Generate random input data\nnp.random.seed(42)\n\n# Generate the first 33 variables using the neural network\nX_first_33 = np.random.normal(loc=0, scale=1, size=(500, 33))\nparameters = initialize_parameters(33, hidden_size, output_size)\nforward = forward_propagation(X_first_33, parameters)\ny_neural_network = forward['A2']\n\n# Generate random values for the remaining variables\nX_rest = np.random.normal(loc=0, scale=1,size=(500, input_size - 33))\n\n# Combine the generated values\nX = np.concatenate((X_first_33, X_rest), axis=1)\n\n# Threshold for binary classification\nthreshold =0.5\ny_binary_neural_network = (y_neural_network > threshold).astype(int)\n\n# Add the generated y values to the existing DataFrame\ndata = pd.DataFrame(X, columns=[f'var{i}' for i in range(input_size)])\ndata['y'] = y_binary_neural_network","metadata":{"execution":{"iopub.status.busy":"2024-06-26T15:16:45.165450Z","iopub.execute_input":"2024-06-26T15:16:45.167022Z","iopub.status.idle":"2024-06-26T15:16:45.212694Z","shell.execute_reply.started":"2024-06-26T15:16:45.166970Z","shell.execute_reply":"2024-06-26T15:16:45.211439Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2. Create knockoff variables","metadata":{}},{"cell_type":"code","source":"def get_arccos(X):\n    # X is a 2-d array\n    n, p = X.shape\n    cos_a = np.zeros([n, n, n])\n    \n    for r in range(n):\n        \n        xr = X[r]\n        X_r = X - xr\n        cross = np.dot(X_r, X_r.T)\n        row_norm = np.sqrt(np.sum(X_r**2, axis = 1))\n        outer_norm = np.outer(row_norm, row_norm)\n        \n        zero_idx = (outer_norm == 0.)\n        outer_norm[zero_idx] = 1.\n        cos_a_kl = cross / outer_norm\n        cos_a_kl[zero_idx] = 0.\n\n        cos_a[:,:,r] = cos_a_kl\n        \n    cos_a[cos_a > 1] = 1.\n    cos_a[cos_a < -1] = -1.\n    a = np.arccos(cos_a)\n\n    a_bar_12 = np.mean(a, axis = 0, keepdims = True)\n    a_bar_02 = np.mean(a, axis = 1, keepdims = True)\n    a_bar_2  = np.mean(a, axis = (0,1), keepdims = True)\n    A = a - a_bar_12 - a_bar_02 + a_bar_2\n        \n    return a, A\n\ndef get_arccos_1d(X):\n    # X is a 1-d array\n    \n    X = np.squeeze(X)\n    Y = X[:,None] - X\n    Z = Y.T[:,:,None]*Y.T[:,None]\n    n = len(X)\n    \n    a = np.zeros([n, n, n])\n    a[Z == 0.] = np.pi/2.\n    a[Z < 0.] = np.pi\n    \n    a = np.transpose(a, (1,2,0))\n    \n    #a = Z[Z>0.]*0. + Z[Z==0.]*np.pi/2. + Z[Z<0.]*np.pi\n\n    a_bar_12 = np.mean(a, axis = 0, keepdims = True)\n    a_bar_02 = np.mean(a, axis = 1, keepdims = True)\n    a_bar_2  = np.mean(a, axis = (0,1), keepdims = True)\n    A = a - a_bar_12 - a_bar_02 + a_bar_2\n    \n    return a, A\n\ndef orthonormalize(X):\n    # X is a 2-d array\n    # output: Gram-Schmidt orthogonalization of X\n    \n    n, p = X.shape\n    Y = np.zeros([n,p])\n    Y[:,0] = X[:,0]/np.sqrt(np.sum(X[:,0]**2))\n    \n    for j in range(1,p):\n        \n        Yj = Y[:,range(j)]\n        xj = X[:,j]\n        w = np.dot(xj, Yj)\n        xj_p = np.sum(w*Yj, axis = 1)\n        yj = xj - xj_p\n        yj = yj/np.sqrt(np.sum(yj**2))\n        \n        Y[:,j] = yj\n        \n    return Y\n\n# Main functions\ndef projection_corr(X, Y):\n    # X, Y are 2-d array\n    \n    nx, p = X.shape\n    ny, q = Y.shape\n    \n    if nx == ny:\n        n = nx\n    else:\n        raise ValueError(\"sample sizes do not match.\")\n        \n    a_x, A_x = get_arccos(X)\n    a_y, A_y = get_arccos(Y)\n    \n    S_xy = np.sum(A_x * A_y) / (n**3)\n    S_xx = np.sum(A_x**2) / (n**3)\n    S_yy = np.sum(A_y**2) / (n**3)\n    \n    if S_xx * S_yy == 0.:\n        corr = 0.\n    else:\n        corr = np.sqrt( S_xy / np.sqrt(S_xx * S_yy) )\n    \n    return corr\n\ndef projection_corr_1d(X, Y):\n    \n    nx, p = X.shape\n    ny, q = Y.shape\n    \n    if nx == ny:\n        n = nx\n    else:\n        raise ValueError(\"sample sizes do not match.\")\n        \n    a_x, A_x = get_arccos_1d(X)\n    a_y, A_y = get_arccos_1d(Y)\n    \n    S_xy = np.sum(A_x * A_y) / (n**3)\n    S_xx = np.sum(A_x**2) / (n**3)\n    S_yy = np.sum(A_y**2) / (n**3)\n    \n    if S_xx * S_yy == 0.:\n        corr = 0.\n    else:\n        corr = np.sqrt( S_xy / np.sqrt(S_xx * S_yy) )\n    \n    return corr\n\ndef projection_corr_1dy(X, Y):\n    \n    nx, p = X.shape\n    ny, q = Y.shape\n    \n    if nx == ny:\n        n = nx\n    else:\n        raise ValueError(\"sample sizes do not match.\")\n        \n    a_x, A_x = get_arccos(X)\n    a_y, A_y = get_arccos_1d(Y)\n    \n    S_xy = np.sum(A_x * A_y) / (n**3)\n    S_xx = np.sum(A_x**2) / (n**3)\n    S_yy = np.sum(A_y**2) / (n**3)\n    \n    if S_xx * S_yy == 0.:\n        corr = 0.\n    else:\n        corr = np.sqrt( S_xy / np.sqrt(S_xx * S_yy) )\n    \n    return corr\n\ndef get_equi_features(X):\n    # X is 2-d array\n    \n    n, p = X.shape\n    scale = np.sqrt(np.sum(X**2, axis=0))\n    Xstd = X / scale\n    sigma = np.dot(Xstd.T, Xstd)\n    sigma_inv = np.linalg.inv(sigma)\n    lambd_min = eigsh(sigma, k=1, which='SA')[0].squeeze()\n    sj = np.min([1., 2.*lambd_min])\n    sj = sj - 0.00001\n    \n    mat_s = np.diag([sj]*p)\n    A = 2*mat_s - sj*sj*sigma_inv\n    C = np.linalg.cholesky(A).T\n    \n    Xn = np.random.randn(n, p)\n    XX = np.hstack([Xstd, Xn])\n    XXo = orthonormalize(XX)\n    U = XXo[:,range(p,2*p)]\n    \n    Xnew = np.dot(Xstd,  np.eye(p) - sigma_inv*sj) + np.dot(U,C)\n    return Xnew","metadata":{"execution":{"iopub.status.busy":"2024-06-26T15:16:47.326954Z","iopub.execute_input":"2024-06-26T15:16:47.327454Z","iopub.status.idle":"2024-06-26T15:16:47.354839Z","shell.execute_reply.started":"2024-06-26T15:16:47.327414Z","shell.execute_reply":"2024-06-26T15:16:47.353543Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X = data.iloc[:,:-1]\nX = (X - X.mean()) / X.std()\nY = data.iloc[:,-1]\nX_knockoff = get_equi_features(X)\nX_knockoff = (X_knockoff - X_knockoff.mean()) / X_knockoff.std()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T15:16:47.612803Z","iopub.execute_input":"2024-06-26T15:16:47.613209Z","iopub.status.idle":"2024-06-26T15:16:47.741613Z","shell.execute_reply.started":"2024-06-26T15:16:47.613167Z","shell.execute_reply":"2024-06-26T15:16:47.740298Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_knockoff_df = pd.DataFrame(X_knockoff)\ncolumn_names = ['feature0_k', 'feature1_k', 'feature2_k', 'feature3_k', 'feature4_k','feature5_k', 'feature6_k', 'feature7_k', 'feature8_k', 'feature9_k',\n                'feature10_k', 'feature11_k', 'feature12_k', 'feature13_k', 'feature14_k','feature15_k', 'feature16_k', 'feature17_k', 'feature18_k', 'feature19_k',\n                'feature20_k', 'feature21_k', 'feature22_k', 'feature23_k', 'feature24_k','feature25_k', 'feature26_k', 'feature27_k', 'feature28_k', 'feature29_k',\n                'feature30_k', 'feature31_k', 'feature32_k', 'feature33_k', 'feature34_k','feature35_k', 'feature36_k', 'feature37_k', 'feature38_k', 'feature39_k',\n                'feature40_k', 'feature41_k', 'feature42_k', 'feature43_k', 'feature44_k','feature45_k', 'feature46_k', 'feature47_k', 'feature48_k', 'feature49_k',\n                'feature50_k', 'feature51_k', 'feature52_k', 'feature53_k', 'feature54_k','feature55_k', 'feature56_k', 'feature57_k', 'feature58_k', 'feature59_k',\n                'feature60_k', 'feature61_k', 'feature62_k', 'feature63_k', 'feature64_k','feature65_k', 'feature66_k', 'feature67_k', 'feature68_k', 'feature69_k',\n                'feature70_k', 'feature71_k', 'feature72_k', 'feature73_k', 'feature74_k','feature75_k', 'feature76_k', 'feature77_k', 'feature78_k', 'feature79_k',\n                'feature80_k', 'feature81_k', 'feature82_k', 'feature83_k', 'feature84_k','feature85_k', 'feature86_k', 'feature87_k', 'feature88_k', 'feature89_k',\n                'feature90_k', 'feature91_k', 'feature92_k', 'feature93_k', 'feature94_k','feature95_k', 'feature96_k', 'feature97_k', 'feature98_k', 'feature99_k']\nX_knockoff_df.columns = column_names\nfeature = pd.concat([X,X_knockoff_df],axis = 1)\ndataset1 =  pd.concat([feature,data['y']],axis = 1)\ndataset1","metadata":{"execution":{"iopub.status.busy":"2024-06-26T15:16:47.771464Z","iopub.execute_input":"2024-06-26T15:16:47.772276Z","iopub.status.idle":"2024-06-26T15:16:47.841369Z","shell.execute_reply.started":"2024-06-26T15:16:47.772213Z","shell.execute_reply":"2024-06-26T15:16:47.840243Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         var0      var1      var2      var3      var4      var5      var6  \\\n0    0.461406 -0.138062  0.678772  1.527778 -0.218823 -0.145966  1.500183   \n1   -1.059556  0.807638 -1.180781  0.244734 -2.025435 -1.232025  0.165041   \n2   -0.095075  0.985780  0.394094 -0.589024  0.404716  1.613266 -0.059700   \n3   -0.254152 -1.395087 -0.384428 -0.293781 -0.813647 -0.073646  0.365155   \n4   -1.064050  0.464173 -0.880810  1.554045 -0.793729 -0.233248  0.760637   \n..        ...       ...       ...       ...       ...       ...       ...   \n495 -2.719838  0.245937 -0.380494 -0.174807 -0.923690 -0.060646  1.319356   \n496 -1.249367  0.019293  0.859484  0.192001  0.329237  1.219954 -0.738992   \n497  0.559587  0.877735  1.646569  1.005932  0.093156 -1.212238  0.125245   \n498 -1.515261  0.175513  0.699545 -1.373012 -0.187130  0.797151 -2.858676   \n499 -1.523851 -0.267471  0.285857  1.783188  0.447981  0.707124  1.099610   \n\n         var7      var8      var9  ...  feature91_k  feature92_k  feature93_k  \\\n0    0.711481 -0.433734  0.501047  ...    -0.979433     0.463950    -0.384183   \n1    0.682626  0.208686 -0.157825  ...    -1.279174     0.652524    -0.121155   \n2    1.505561 -2.589300  0.780671  ...     0.694236     0.497791     0.024633   \n3    1.825841  0.211904  0.215750  ...    -2.084411     1.355300    -0.607549   \n4   -1.278976  0.264916  1.266400  ...     1.040937    -1.547896     1.017070   \n..        ...       ...       ...  ...          ...          ...          ...   \n495 -0.497589  1.004847 -0.849204  ...    -1.261415     1.671708     1.207839   \n496 -0.272049  1.675527 -0.239070  ...     1.068995    -1.610087     1.177215   \n497  0.202628  0.747672  1.313196  ...     0.004768     0.109657    -0.461855   \n498  1.277270  0.391446  0.675390  ...     0.115898     0.706394     0.489416   \n499 -1.013814 -0.128593  2.385796  ...     0.935980    -0.723933     0.606612   \n\n     feature94_k  feature95_k  feature96_k  feature97_k  feature98_k  \\\n0       0.512113     0.015657    -0.690012    -0.558037    -0.565508   \n1      -0.345293     1.321991     0.690405     0.350790     0.369794   \n2       0.594199    -0.417800    -1.647849     0.540547     0.611145   \n3      -0.210499     1.020312    -0.815541    -1.246425    -1.357394   \n4      -0.096840     2.970988    -1.034747     0.388984     1.671235   \n..           ...          ...          ...          ...          ...   \n495     3.024718     3.153122     0.406847    -0.757302    -1.000496   \n496    -1.067030    -0.069279    -0.856657     0.215517    -0.481231   \n497     1.119496     0.730847    -1.192008     1.568252    -0.299782   \n498    -1.144433     1.015803     0.695032     0.517188    -1.165706   \n499    -0.719025     0.326483    -0.438186    -0.851876     1.040913   \n\n     feature99_k  y  \n0       2.721553  1  \n1       0.931040  0  \n2      -0.013600  1  \n3       0.573031  0  \n4      -0.286414  0  \n..           ... ..  \n495     0.115265  0  \n496     1.749924  0  \n497    -0.661093  1  \n498     2.305960  0  \n499     0.003380  0  \n\n[500 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var0</th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n      <th>var7</th>\n      <th>var8</th>\n      <th>var9</th>\n      <th>...</th>\n      <th>feature91_k</th>\n      <th>feature92_k</th>\n      <th>feature93_k</th>\n      <th>feature94_k</th>\n      <th>feature95_k</th>\n      <th>feature96_k</th>\n      <th>feature97_k</th>\n      <th>feature98_k</th>\n      <th>feature99_k</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.461406</td>\n      <td>-0.138062</td>\n      <td>0.678772</td>\n      <td>1.527778</td>\n      <td>-0.218823</td>\n      <td>-0.145966</td>\n      <td>1.500183</td>\n      <td>0.711481</td>\n      <td>-0.433734</td>\n      <td>0.501047</td>\n      <td>...</td>\n      <td>-0.979433</td>\n      <td>0.463950</td>\n      <td>-0.384183</td>\n      <td>0.512113</td>\n      <td>0.015657</td>\n      <td>-0.690012</td>\n      <td>-0.558037</td>\n      <td>-0.565508</td>\n      <td>2.721553</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.059556</td>\n      <td>0.807638</td>\n      <td>-1.180781</td>\n      <td>0.244734</td>\n      <td>-2.025435</td>\n      <td>-1.232025</td>\n      <td>0.165041</td>\n      <td>0.682626</td>\n      <td>0.208686</td>\n      <td>-0.157825</td>\n      <td>...</td>\n      <td>-1.279174</td>\n      <td>0.652524</td>\n      <td>-0.121155</td>\n      <td>-0.345293</td>\n      <td>1.321991</td>\n      <td>0.690405</td>\n      <td>0.350790</td>\n      <td>0.369794</td>\n      <td>0.931040</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.095075</td>\n      <td>0.985780</td>\n      <td>0.394094</td>\n      <td>-0.589024</td>\n      <td>0.404716</td>\n      <td>1.613266</td>\n      <td>-0.059700</td>\n      <td>1.505561</td>\n      <td>-2.589300</td>\n      <td>0.780671</td>\n      <td>...</td>\n      <td>0.694236</td>\n      <td>0.497791</td>\n      <td>0.024633</td>\n      <td>0.594199</td>\n      <td>-0.417800</td>\n      <td>-1.647849</td>\n      <td>0.540547</td>\n      <td>0.611145</td>\n      <td>-0.013600</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.254152</td>\n      <td>-1.395087</td>\n      <td>-0.384428</td>\n      <td>-0.293781</td>\n      <td>-0.813647</td>\n      <td>-0.073646</td>\n      <td>0.365155</td>\n      <td>1.825841</td>\n      <td>0.211904</td>\n      <td>0.215750</td>\n      <td>...</td>\n      <td>-2.084411</td>\n      <td>1.355300</td>\n      <td>-0.607549</td>\n      <td>-0.210499</td>\n      <td>1.020312</td>\n      <td>-0.815541</td>\n      <td>-1.246425</td>\n      <td>-1.357394</td>\n      <td>0.573031</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.064050</td>\n      <td>0.464173</td>\n      <td>-0.880810</td>\n      <td>1.554045</td>\n      <td>-0.793729</td>\n      <td>-0.233248</td>\n      <td>0.760637</td>\n      <td>-1.278976</td>\n      <td>0.264916</td>\n      <td>1.266400</td>\n      <td>...</td>\n      <td>1.040937</td>\n      <td>-1.547896</td>\n      <td>1.017070</td>\n      <td>-0.096840</td>\n      <td>2.970988</td>\n      <td>-1.034747</td>\n      <td>0.388984</td>\n      <td>1.671235</td>\n      <td>-0.286414</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>-2.719838</td>\n      <td>0.245937</td>\n      <td>-0.380494</td>\n      <td>-0.174807</td>\n      <td>-0.923690</td>\n      <td>-0.060646</td>\n      <td>1.319356</td>\n      <td>-0.497589</td>\n      <td>1.004847</td>\n      <td>-0.849204</td>\n      <td>...</td>\n      <td>-1.261415</td>\n      <td>1.671708</td>\n      <td>1.207839</td>\n      <td>3.024718</td>\n      <td>3.153122</td>\n      <td>0.406847</td>\n      <td>-0.757302</td>\n      <td>-1.000496</td>\n      <td>0.115265</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>-1.249367</td>\n      <td>0.019293</td>\n      <td>0.859484</td>\n      <td>0.192001</td>\n      <td>0.329237</td>\n      <td>1.219954</td>\n      <td>-0.738992</td>\n      <td>-0.272049</td>\n      <td>1.675527</td>\n      <td>-0.239070</td>\n      <td>...</td>\n      <td>1.068995</td>\n      <td>-1.610087</td>\n      <td>1.177215</td>\n      <td>-1.067030</td>\n      <td>-0.069279</td>\n      <td>-0.856657</td>\n      <td>0.215517</td>\n      <td>-0.481231</td>\n      <td>1.749924</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>0.559587</td>\n      <td>0.877735</td>\n      <td>1.646569</td>\n      <td>1.005932</td>\n      <td>0.093156</td>\n      <td>-1.212238</td>\n      <td>0.125245</td>\n      <td>0.202628</td>\n      <td>0.747672</td>\n      <td>1.313196</td>\n      <td>...</td>\n      <td>0.004768</td>\n      <td>0.109657</td>\n      <td>-0.461855</td>\n      <td>1.119496</td>\n      <td>0.730847</td>\n      <td>-1.192008</td>\n      <td>1.568252</td>\n      <td>-0.299782</td>\n      <td>-0.661093</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>-1.515261</td>\n      <td>0.175513</td>\n      <td>0.699545</td>\n      <td>-1.373012</td>\n      <td>-0.187130</td>\n      <td>0.797151</td>\n      <td>-2.858676</td>\n      <td>1.277270</td>\n      <td>0.391446</td>\n      <td>0.675390</td>\n      <td>...</td>\n      <td>0.115898</td>\n      <td>0.706394</td>\n      <td>0.489416</td>\n      <td>-1.144433</td>\n      <td>1.015803</td>\n      <td>0.695032</td>\n      <td>0.517188</td>\n      <td>-1.165706</td>\n      <td>2.305960</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-1.523851</td>\n      <td>-0.267471</td>\n      <td>0.285857</td>\n      <td>1.783188</td>\n      <td>0.447981</td>\n      <td>0.707124</td>\n      <td>1.099610</td>\n      <td>-1.013814</td>\n      <td>-0.128593</td>\n      <td>2.385796</td>\n      <td>...</td>\n      <td>0.935980</td>\n      <td>-0.723933</td>\n      <td>0.606612</td>\n      <td>-0.719025</td>\n      <td>0.326483</td>\n      <td>-0.438186</td>\n      <td>-0.851876</td>\n      <td>1.040913</td>\n      <td>0.003380</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 201 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3.Variables selection","metadata":{}},{"cell_type":"markdown","source":"### proximal gradient descent","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm import tqdm\n\n# Define the proximal gradient descent function\ndef proximal_gradient_descent(W, grad, learning_rate, lambda_reg):\n    return W - learning_rate * (grad + lambda_reg * np.sign(W))\n\n# Initialize lambda arrays\nlambda_array = np.zeros((200, 264))\nlambda_array2 = np.zeros((264, 1))\ninput_dim = 200\n\n# Custom training loop to apply proximal gradient descent\ndef custom_train(model, feature, Y, epochs, batch_size, learning_rate, lambda_reg):\n    for epoch in range(epochs):\n        for start in range(0, len(feature), batch_size):\n            end = start + batch_size\n            x_batch = feature[start:end]\n            y_batch = Y[start:end]\n            \n            with tf.GradientTape() as tape:\n                predictions = model(x_batch, training=True)\n                # Ensure y_batch is kept as 2D shape to match predictions' shape\n                loss = tf.keras.losses.binary_crossentropy(tf.expand_dims(y_batch, axis=-1), predictions)\n                \n            grads = tape.gradient(loss, model.trainable_variables)\n            \n            new_weights = []\n            for w, grad in zip(model.trainable_variables, grads):\n                new_weight = proximal_gradient_descent(w.numpy(), grad.numpy(), learning_rate, lambda_reg)\n                new_weights.append(new_weight)\n                \n            for var, new_weight in zip(model.trainable_variables, new_weights):\n                var.assign(new_weight)\n\n\n\n# Use tqdm for a one-line progress bar\nfor i in tqdm(np.arange(0, 7, 0.05)):\n    model = keras.Sequential()\n    model.add(layers.Dense(264, activation='relu', input_shape=(input_dim,)))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    # Custom training\n    custom_train(model, feature, Y, epochs=50, batch_size=32, learning_rate=0.001, lambda_reg=i)\n\n    first_layer = model.layers[0]\n    weights, biases = first_layer.get_weights()\n    \n    for j in range(min(len(weights), len(lambda_array))):\n        for n in range(min(len(weights[j]), len(lambda_array[j]))):\n            if abs(weights[j][n]) < 5e-4 and lambda_array[j][n] == 0:\n                lambda_array[j][n] = i\n    \n    second_layer = model.layers[1]\n    weights, biases = second_layer.get_weights()\n    for j in range(len(weights)):\n        for n in range(len(weights[j])):\n            if abs(weights[j][n]) < 5e-4 and lambda_array2[j][n] == 0:\n                lambda_array2[j][n] = i\n\nprint(lambda_array)\nnon_zero_lambda_array = lambda_array[np.nonzero(lambda_array)]\nmin_non_zero_lambda = np.min(np.abs(non_zero_lambda_array))\nmax_lambda = np.max(abs(lambda_array))\nprint(f\"min_lambda (except 0): {min_non_zero_lambda}\")\nprint(f\"max_lambda: {max_lambda}\")\ncount_min_non_zero_lambda = np.count_nonzero(np.abs(lambda_array) == 0)\nprint(f\"number of 0: {count_min_non_zero_lambda}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:30:40.789783Z","iopub.execute_input":"2024-06-26T16:30:40.790184Z","iopub.status.idle":"2024-06-26T16:52:07.363814Z","shell.execute_reply.started":"2024-06-26T16:30:40.790152Z","shell.execute_reply":"2024-06-26T16:52:07.362589Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 140/140 [21:26<00:00,  9.19s/it]","output_type":"stream"},{"name":"stdout","text":"[[0.1  0.1  0.1  ... 0.15 0.05 0.1 ]\n [0.15 0.1  0.15 ... 0.1  0.1  0.05]\n [0.15 0.15 0.15 ... 0.05 0.15 0.05]\n ...\n [0.05 0.15 0.15 ... 0.15 0.1  0.1 ]\n [0.1  0.05 0.1  ... 0.05 0.05 0.1 ]\n [0.1  0.1  0.1  ... 0.1  0.05 0.1 ]]\nmin_lambda (except 0): 0.05\nmax_lambda: 0.30000000000000004\nnumber of 0: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef compute_W1(lambda_array):\n    lambda_array = np.where(lambda_array == 0, 8, lambda_array)\n    row_sums1 = np.sum(lambda_array, axis=1)\n    W1 = []\n    for i in range(int(len(row_sums1) / 2)):\n        if row_sums1[i] >= row_sums1[i + 100]:\n            W1.append(row_sums1[i])\n        else:\n            W1.append(-row_sums1[i + 100])\n    return W1, row_sums1\n\ndef select_variables1(W, target_q):\n    T = []\n    for i in W:\n        count1 = 0\n        count2 = 0\n        t = abs(i)\n        for j in W:\n            if j < -t:\n                count1 += 1\n            if j >= t:\n                count2 += 1\n        q = count1 / max(count2, 1)\n        if q <= target_q:\n            T.append(t)\n    if len(T) == 0:\n        return [], None\n    threshold = min(T)\n    selected_vars = [index + 1 for index, value in enumerate(W) if value >= threshold]\n    return selected_vars, threshold\n\n# Sample usage\nW1, row_sums1 = compute_W1(lambda_array)\nselected_vars1, threshold_value1 = select_variables1(W1, 0.01)\nprint(selected_vars1, threshold_value1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:07:20.061934Z","iopub.execute_input":"2024-06-26T17:07:20.062372Z","iopub.status.idle":"2024-06-26T17:07:20.079011Z","shell.execute_reply.started":"2024-06-26T17:07:20.062335Z","shell.execute_reply":"2024-06-26T17:07:20.077594Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[7, 11, 17, 24, 25, 27, 54, 86] 26.200000000000003\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef compute_W2(model, lambda_array):\n    weight_list = [layer.get_weights()[0] for layer in model.layers if 'dense' in layer.name]\n    w_m = weight_list[0]\n    for weight in weight_list[1:]:\n        w_m = np.dot(w_m, weight)\n    lambda_array_adj = np.where(lambda_array == 0, 2, lambda_array)\n    row_sums = np.sum(lambda_array_adj, axis=1)\n    Z = [row_sums[i] * w_m[i] for i in range(len(w_m))]\n    W = [Z[i] ** 2 / 10000 - Z[i + 100] ** 2 / 10000 for i in range(int(len(Z) / 2))]\n    return W, Z\n\ndef select_variables2(W, target_q):\n    T = []\n    for i in W:\n        count1 = len([j for j in W if j < -abs(i)])\n        count2 = len([j for j in W if j >= abs(i)])\n        q = count1 / max(count2, 1)\n        if q < target_q:\n            T.append(abs(i))\n    if len(T) == 0:\n        return [], None\n    threshold = min(T)\n    selected_vars = [index + 1 for index, value in enumerate(W) if value >= threshold]\n    return selected_vars, threshold\n\n# Sample usage\nmodel_2 = keras.Sequential()\nmodel_2.add(layers.Dense(264, activation='relu'))\nmodel_2.add(layers.Dense(1, activation='sigmoid'))\nmodel_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_2.fit(feature, Y, epochs=50, batch_size=32, verbose=0)\n\nW2, Z = compute_W2(model_2, lambda_array)\nselected_vars2, threshold_value2 = select_variables2(W2, 0.01)\nprint(selected_vars2, threshold_value2)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:07:23.229364Z","iopub.execute_input":"2024-06-26T17:07:23.230939Z","iopub.status.idle":"2024-06-26T17:07:27.608219Z","shell.execute_reply.started":"2024-06-26T17:07:23.230895Z","shell.execute_reply":"2024-06-26T17:07:27.606588Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[2, 7, 8, 11, 17, 21, 24, 25, 27, 28, 31, 32] [0.08338176]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}