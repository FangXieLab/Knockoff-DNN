{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8769423,"sourceType":"datasetVersion","datasetId":5269632},{"sourceId":8769844,"sourceType":"datasetVersion","datasetId":5269952}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.models import Sequential\nimport time\nfrom tensorflow.keras.layers import Dense\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:48:34.381112Z","iopub.execute_input":"2024-06-24T03:48:34.381572Z","iopub.status.idle":"2024-06-24T03:48:34.388805Z","shell.execute_reply.started":"2024-06-24T03:48:34.381537Z","shell.execute_reply":"2024-06-24T03:48:34.387448Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Simulation data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/simulationdata/Simulationdata.csv\",index_col= 0)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:52:40.237306Z","iopub.execute_input":"2024-06-24T03:52:40.237751Z","iopub.status.idle":"2024-06-24T03:52:40.322480Z","shell.execute_reply.started":"2024-06-24T03:52:40.237717Z","shell.execute_reply":"2024-06-24T03:52:40.321111Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"         var0      var1      var2      var3      var4      var5      var6  \\\n0    0.461406 -0.138062  0.678772  1.527778 -0.218823 -0.145966  1.500183   \n1   -1.059556  0.807638 -1.180781  0.244734 -2.025435 -1.232025  0.165041   \n2   -0.095075  0.985780  0.394094 -0.589024  0.404716  1.613266 -0.059700   \n3   -0.254152 -1.395087 -0.384428 -0.293781 -0.813647 -0.073646  0.365155   \n4   -1.064050  0.464173 -0.880810  1.554045 -0.793729 -0.233248  0.760637   \n..        ...       ...       ...       ...       ...       ...       ...   \n495 -2.719838  0.245937 -0.380494 -0.174807 -0.923690 -0.060646  1.319356   \n496 -1.249367  0.019293  0.859484  0.192001  0.329237  1.219954 -0.738992   \n497  0.559587  0.877735  1.646569  1.005932  0.093156 -1.212238  0.125245   \n498 -1.515261  0.175513  0.699545 -1.373012 -0.187130  0.797151 -2.858676   \n499 -1.523851 -0.267471  0.285857  1.783188  0.447981  0.707124  1.099610   \n\n         var7      var8      var9  ...  feature91_k  feature92_k  feature93_k  \\\n0    0.711481 -0.433734  0.501047  ...    -0.979433     0.463950    -0.384183   \n1    0.682626  0.208686 -0.157825  ...    -1.279174     0.652524    -0.121155   \n2    1.505561 -2.589300  0.780671  ...     0.694236     0.497791     0.024633   \n3    1.825841  0.211904  0.215750  ...    -2.084411     1.355300    -0.607549   \n4   -1.278976  0.264916  1.266400  ...     1.040937    -1.547896     1.017070   \n..        ...       ...       ...  ...          ...          ...          ...   \n495 -0.497589  1.004847 -0.849204  ...    -1.261415     1.671708     1.207839   \n496 -0.272049  1.675527 -0.239070  ...     1.068995    -1.610087     1.177215   \n497  0.202628  0.747672  1.313196  ...     0.004768     0.109657    -0.461855   \n498  1.277270  0.391446  0.675390  ...     0.115898     0.706394     0.489416   \n499 -1.013814 -0.128593  2.385796  ...     0.935980    -0.723933     0.606612   \n\n     feature94_k  feature95_k  feature96_k  feature97_k  feature98_k  \\\n0       0.512113     0.015657    -0.690012    -0.558037    -0.565508   \n1      -0.345293     1.321991     0.690405     0.350790     0.369794   \n2       0.594199    -0.417800    -1.647849     0.540547     0.611145   \n3      -0.210499     1.020312    -0.815541    -1.246425    -1.357394   \n4      -0.096840     2.970988    -1.034747     0.388984     1.671235   \n..           ...          ...          ...          ...          ...   \n495     3.024718     3.153122     0.406847    -0.757302    -1.000496   \n496    -1.067030    -0.069279    -0.856657     0.215517    -0.481231   \n497     1.119496     0.730847    -1.192008     1.568252    -0.299782   \n498    -1.144433     1.015803     0.695032     0.517188    -1.165706   \n499    -0.719025     0.326483    -0.438186    -0.851876     1.040913   \n\n     feature99_k  y  \n0       2.721553  1  \n1       0.931040  0  \n2      -0.013600  1  \n3       0.573031  0  \n4      -0.286414  0  \n..           ... ..  \n495     0.115265  0  \n496     1.749924  0  \n497    -0.661093  1  \n498     2.305960  0  \n499     0.003380  0  \n\n[500 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var0</th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n      <th>var7</th>\n      <th>var8</th>\n      <th>var9</th>\n      <th>...</th>\n      <th>feature91_k</th>\n      <th>feature92_k</th>\n      <th>feature93_k</th>\n      <th>feature94_k</th>\n      <th>feature95_k</th>\n      <th>feature96_k</th>\n      <th>feature97_k</th>\n      <th>feature98_k</th>\n      <th>feature99_k</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.461406</td>\n      <td>-0.138062</td>\n      <td>0.678772</td>\n      <td>1.527778</td>\n      <td>-0.218823</td>\n      <td>-0.145966</td>\n      <td>1.500183</td>\n      <td>0.711481</td>\n      <td>-0.433734</td>\n      <td>0.501047</td>\n      <td>...</td>\n      <td>-0.979433</td>\n      <td>0.463950</td>\n      <td>-0.384183</td>\n      <td>0.512113</td>\n      <td>0.015657</td>\n      <td>-0.690012</td>\n      <td>-0.558037</td>\n      <td>-0.565508</td>\n      <td>2.721553</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.059556</td>\n      <td>0.807638</td>\n      <td>-1.180781</td>\n      <td>0.244734</td>\n      <td>-2.025435</td>\n      <td>-1.232025</td>\n      <td>0.165041</td>\n      <td>0.682626</td>\n      <td>0.208686</td>\n      <td>-0.157825</td>\n      <td>...</td>\n      <td>-1.279174</td>\n      <td>0.652524</td>\n      <td>-0.121155</td>\n      <td>-0.345293</td>\n      <td>1.321991</td>\n      <td>0.690405</td>\n      <td>0.350790</td>\n      <td>0.369794</td>\n      <td>0.931040</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.095075</td>\n      <td>0.985780</td>\n      <td>0.394094</td>\n      <td>-0.589024</td>\n      <td>0.404716</td>\n      <td>1.613266</td>\n      <td>-0.059700</td>\n      <td>1.505561</td>\n      <td>-2.589300</td>\n      <td>0.780671</td>\n      <td>...</td>\n      <td>0.694236</td>\n      <td>0.497791</td>\n      <td>0.024633</td>\n      <td>0.594199</td>\n      <td>-0.417800</td>\n      <td>-1.647849</td>\n      <td>0.540547</td>\n      <td>0.611145</td>\n      <td>-0.013600</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.254152</td>\n      <td>-1.395087</td>\n      <td>-0.384428</td>\n      <td>-0.293781</td>\n      <td>-0.813647</td>\n      <td>-0.073646</td>\n      <td>0.365155</td>\n      <td>1.825841</td>\n      <td>0.211904</td>\n      <td>0.215750</td>\n      <td>...</td>\n      <td>-2.084411</td>\n      <td>1.355300</td>\n      <td>-0.607549</td>\n      <td>-0.210499</td>\n      <td>1.020312</td>\n      <td>-0.815541</td>\n      <td>-1.246425</td>\n      <td>-1.357394</td>\n      <td>0.573031</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.064050</td>\n      <td>0.464173</td>\n      <td>-0.880810</td>\n      <td>1.554045</td>\n      <td>-0.793729</td>\n      <td>-0.233248</td>\n      <td>0.760637</td>\n      <td>-1.278976</td>\n      <td>0.264916</td>\n      <td>1.266400</td>\n      <td>...</td>\n      <td>1.040937</td>\n      <td>-1.547896</td>\n      <td>1.017070</td>\n      <td>-0.096840</td>\n      <td>2.970988</td>\n      <td>-1.034747</td>\n      <td>0.388984</td>\n      <td>1.671235</td>\n      <td>-0.286414</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>-2.719838</td>\n      <td>0.245937</td>\n      <td>-0.380494</td>\n      <td>-0.174807</td>\n      <td>-0.923690</td>\n      <td>-0.060646</td>\n      <td>1.319356</td>\n      <td>-0.497589</td>\n      <td>1.004847</td>\n      <td>-0.849204</td>\n      <td>...</td>\n      <td>-1.261415</td>\n      <td>1.671708</td>\n      <td>1.207839</td>\n      <td>3.024718</td>\n      <td>3.153122</td>\n      <td>0.406847</td>\n      <td>-0.757302</td>\n      <td>-1.000496</td>\n      <td>0.115265</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>-1.249367</td>\n      <td>0.019293</td>\n      <td>0.859484</td>\n      <td>0.192001</td>\n      <td>0.329237</td>\n      <td>1.219954</td>\n      <td>-0.738992</td>\n      <td>-0.272049</td>\n      <td>1.675527</td>\n      <td>-0.239070</td>\n      <td>...</td>\n      <td>1.068995</td>\n      <td>-1.610087</td>\n      <td>1.177215</td>\n      <td>-1.067030</td>\n      <td>-0.069279</td>\n      <td>-0.856657</td>\n      <td>0.215517</td>\n      <td>-0.481231</td>\n      <td>1.749924</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>0.559587</td>\n      <td>0.877735</td>\n      <td>1.646569</td>\n      <td>1.005932</td>\n      <td>0.093156</td>\n      <td>-1.212238</td>\n      <td>0.125245</td>\n      <td>0.202628</td>\n      <td>0.747672</td>\n      <td>1.313196</td>\n      <td>...</td>\n      <td>0.004768</td>\n      <td>0.109657</td>\n      <td>-0.461855</td>\n      <td>1.119496</td>\n      <td>0.730847</td>\n      <td>-1.192008</td>\n      <td>1.568252</td>\n      <td>-0.299782</td>\n      <td>-0.661093</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>-1.515261</td>\n      <td>0.175513</td>\n      <td>0.699545</td>\n      <td>-1.373012</td>\n      <td>-0.187130</td>\n      <td>0.797151</td>\n      <td>-2.858676</td>\n      <td>1.277270</td>\n      <td>0.391446</td>\n      <td>0.675390</td>\n      <td>...</td>\n      <td>0.115898</td>\n      <td>0.706394</td>\n      <td>0.489416</td>\n      <td>-1.144433</td>\n      <td>1.015803</td>\n      <td>0.695032</td>\n      <td>0.517188</td>\n      <td>-1.165706</td>\n      <td>2.305960</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-1.523851</td>\n      <td>-0.267471</td>\n      <td>0.285857</td>\n      <td>1.783188</td>\n      <td>0.447981</td>\n      <td>0.707124</td>\n      <td>1.099610</td>\n      <td>-1.013814</td>\n      <td>-0.128593</td>\n      <td>2.385796</td>\n      <td>...</td>\n      <td>0.935980</td>\n      <td>-0.723933</td>\n      <td>0.606612</td>\n      <td>-0.719025</td>\n      <td>0.326483</td>\n      <td>-0.438186</td>\n      <td>-0.851876</td>\n      <td>1.040913</td>\n      <td>0.003380</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 201 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.autograd as autograd\n\n\ndef add_dimension_glasso(var, dim=0):\n    return var.pow(2).sum(dim=dim).add(1e-8).pow(1/2.).sum()\n\n\ndef deep_lasso_regularizer(loss, inputs):\n    grad_params = autograd.grad(loss, inputs, create_graph=True, allow_unused=True)\n    regval = add_dimension_glasso(grad_params[0], dim=0)\n    return regval\n\n\ndef get_feat_importance_deeplasso(net, testloader, criterion, device):\n    net.eval()\n    grads = []\n    for batch_idx, (inputs_num, inputs_cat, targets) in enumerate(testloader):\n        inputs_num, inputs_cat, targets = inputs_num.to(device).float(), inputs_cat.to(device), targets.to(device)\n        inputs_num, inputs_cat = inputs_num if inputs_num.nelement() != 0 else None, \\\n                                    inputs_cat if inputs_cat.nelement() != 0 else None\n        inputs_num.requires_grad_()\n        outputs = net(inputs_num, inputs_cat)\n        loss = criterion(outputs, targets)\n        grad_params = autograd.grad(loss, inputs_num, create_graph=True, allow_unused=True)\n        grads.append(grad_params[0].detach().cpu())\n\n    grads = torch.cat(grads)\n    importances = grads.pow(2).sum(dim=0).pow(1/2.)\n    return importances","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:26:38.271123Z","iopub.execute_input":"2024-06-24T03:26:38.271601Z","iopub.status.idle":"2024-06-24T03:26:40.575909Z","shell.execute_reply.started":"2024-06-24T03:26:38.271568Z","shell.execute_reply":"2024-06-24T03:26:40.574736Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_re = data.iloc[:,:100]\ndata_re","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:52:42.524180Z","iopub.execute_input":"2024-06-24T03:52:42.524597Z","iopub.status.idle":"2024-06-24T03:52:42.558846Z","shell.execute_reply.started":"2024-06-24T03:52:42.524568Z","shell.execute_reply":"2024-06-24T03:52:42.557740Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"         var0      var1      var2      var3      var4      var5      var6  \\\n0    0.461406 -0.138062  0.678772  1.527778 -0.218823 -0.145966  1.500183   \n1   -1.059556  0.807638 -1.180781  0.244734 -2.025435 -1.232025  0.165041   \n2   -0.095075  0.985780  0.394094 -0.589024  0.404716  1.613266 -0.059700   \n3   -0.254152 -1.395087 -0.384428 -0.293781 -0.813647 -0.073646  0.365155   \n4   -1.064050  0.464173 -0.880810  1.554045 -0.793729 -0.233248  0.760637   \n..        ...       ...       ...       ...       ...       ...       ...   \n495 -2.719838  0.245937 -0.380494 -0.174807 -0.923690 -0.060646  1.319356   \n496 -1.249367  0.019293  0.859484  0.192001  0.329237  1.219954 -0.738992   \n497  0.559587  0.877735  1.646569  1.005932  0.093156 -1.212238  0.125245   \n498 -1.515261  0.175513  0.699545 -1.373012 -0.187130  0.797151 -2.858676   \n499 -1.523851 -0.267471  0.285857  1.783188  0.447981  0.707124  1.099610   \n\n         var7      var8      var9  ...     var90     var91     var92  \\\n0    0.711481 -0.433734  0.501047  ... -1.139589 -0.118937  0.461903   \n1    0.682626  0.208686 -0.157825  ... -0.366453  0.927959  0.801559   \n2    1.505561 -2.589300  0.780671  ... -0.972883  0.181779  0.408541   \n3    1.825841  0.211904  0.215750  ...  1.032993 -1.396659  0.603730   \n4   -1.278976  0.264916  1.266400  ...  0.728636  0.442393 -1.713243   \n..        ...       ...       ...  ...       ...       ...       ...   \n495 -0.497589  1.004847 -0.849204  ...  0.409179 -1.047576  2.274519   \n496 -0.272049  1.675527 -0.239070  ... -0.315627 -1.131946 -0.243462   \n497  0.202628  0.747672  1.313196  ... -0.453492  0.024436 -1.320878   \n498  1.277270  0.391446  0.675390  ... -0.763045 -1.129448  0.636055   \n499 -1.013814 -0.128593  2.385796  ... -1.269593  1.183064 -0.421932   \n\n        var93     var94     var95     var96     var97     var98     var99  \n0    0.033938  1.317960  0.851600 -0.061595  0.272244  0.350472  0.385494  \n1    1.383283 -1.018771 -0.336806  0.735571  0.514277  1.590418  0.402996  \n2   -1.101319  0.016053  0.699702 -1.251158  1.201414 -0.324483  1.252936  \n3   -1.675585 -0.866563 -1.261675  0.759452 -0.022832  0.040496 -1.147744  \n4    0.999105  0.908532  0.911316  0.333963  0.275905  1.089121  1.929070  \n..        ...       ...       ...       ...       ...       ...       ...  \n495  1.565375  2.044504  2.358369 -0.713754 -0.879213  2.085462 -0.167242  \n496  0.005096  1.240280 -1.078799  0.317837  0.626911  0.032810 -0.529699  \n497  1.907564  0.905234 -0.718263 -1.216225 -1.042418  0.642308 -1.049911  \n498  0.906020  1.198538 -0.257622  0.434496  0.940782 -1.300555  0.258037  \n499  0.058725 -0.629062 -0.277465  0.184457  0.814573  0.957620  0.415154  \n\n[500 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var0</th>\n      <th>var1</th>\n      <th>var2</th>\n      <th>var3</th>\n      <th>var4</th>\n      <th>var5</th>\n      <th>var6</th>\n      <th>var7</th>\n      <th>var8</th>\n      <th>var9</th>\n      <th>...</th>\n      <th>var90</th>\n      <th>var91</th>\n      <th>var92</th>\n      <th>var93</th>\n      <th>var94</th>\n      <th>var95</th>\n      <th>var96</th>\n      <th>var97</th>\n      <th>var98</th>\n      <th>var99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.461406</td>\n      <td>-0.138062</td>\n      <td>0.678772</td>\n      <td>1.527778</td>\n      <td>-0.218823</td>\n      <td>-0.145966</td>\n      <td>1.500183</td>\n      <td>0.711481</td>\n      <td>-0.433734</td>\n      <td>0.501047</td>\n      <td>...</td>\n      <td>-1.139589</td>\n      <td>-0.118937</td>\n      <td>0.461903</td>\n      <td>0.033938</td>\n      <td>1.317960</td>\n      <td>0.851600</td>\n      <td>-0.061595</td>\n      <td>0.272244</td>\n      <td>0.350472</td>\n      <td>0.385494</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.059556</td>\n      <td>0.807638</td>\n      <td>-1.180781</td>\n      <td>0.244734</td>\n      <td>-2.025435</td>\n      <td>-1.232025</td>\n      <td>0.165041</td>\n      <td>0.682626</td>\n      <td>0.208686</td>\n      <td>-0.157825</td>\n      <td>...</td>\n      <td>-0.366453</td>\n      <td>0.927959</td>\n      <td>0.801559</td>\n      <td>1.383283</td>\n      <td>-1.018771</td>\n      <td>-0.336806</td>\n      <td>0.735571</td>\n      <td>0.514277</td>\n      <td>1.590418</td>\n      <td>0.402996</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.095075</td>\n      <td>0.985780</td>\n      <td>0.394094</td>\n      <td>-0.589024</td>\n      <td>0.404716</td>\n      <td>1.613266</td>\n      <td>-0.059700</td>\n      <td>1.505561</td>\n      <td>-2.589300</td>\n      <td>0.780671</td>\n      <td>...</td>\n      <td>-0.972883</td>\n      <td>0.181779</td>\n      <td>0.408541</td>\n      <td>-1.101319</td>\n      <td>0.016053</td>\n      <td>0.699702</td>\n      <td>-1.251158</td>\n      <td>1.201414</td>\n      <td>-0.324483</td>\n      <td>1.252936</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.254152</td>\n      <td>-1.395087</td>\n      <td>-0.384428</td>\n      <td>-0.293781</td>\n      <td>-0.813647</td>\n      <td>-0.073646</td>\n      <td>0.365155</td>\n      <td>1.825841</td>\n      <td>0.211904</td>\n      <td>0.215750</td>\n      <td>...</td>\n      <td>1.032993</td>\n      <td>-1.396659</td>\n      <td>0.603730</td>\n      <td>-1.675585</td>\n      <td>-0.866563</td>\n      <td>-1.261675</td>\n      <td>0.759452</td>\n      <td>-0.022832</td>\n      <td>0.040496</td>\n      <td>-1.147744</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.064050</td>\n      <td>0.464173</td>\n      <td>-0.880810</td>\n      <td>1.554045</td>\n      <td>-0.793729</td>\n      <td>-0.233248</td>\n      <td>0.760637</td>\n      <td>-1.278976</td>\n      <td>0.264916</td>\n      <td>1.266400</td>\n      <td>...</td>\n      <td>0.728636</td>\n      <td>0.442393</td>\n      <td>-1.713243</td>\n      <td>0.999105</td>\n      <td>0.908532</td>\n      <td>0.911316</td>\n      <td>0.333963</td>\n      <td>0.275905</td>\n      <td>1.089121</td>\n      <td>1.929070</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>-2.719838</td>\n      <td>0.245937</td>\n      <td>-0.380494</td>\n      <td>-0.174807</td>\n      <td>-0.923690</td>\n      <td>-0.060646</td>\n      <td>1.319356</td>\n      <td>-0.497589</td>\n      <td>1.004847</td>\n      <td>-0.849204</td>\n      <td>...</td>\n      <td>0.409179</td>\n      <td>-1.047576</td>\n      <td>2.274519</td>\n      <td>1.565375</td>\n      <td>2.044504</td>\n      <td>2.358369</td>\n      <td>-0.713754</td>\n      <td>-0.879213</td>\n      <td>2.085462</td>\n      <td>-0.167242</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>-1.249367</td>\n      <td>0.019293</td>\n      <td>0.859484</td>\n      <td>0.192001</td>\n      <td>0.329237</td>\n      <td>1.219954</td>\n      <td>-0.738992</td>\n      <td>-0.272049</td>\n      <td>1.675527</td>\n      <td>-0.239070</td>\n      <td>...</td>\n      <td>-0.315627</td>\n      <td>-1.131946</td>\n      <td>-0.243462</td>\n      <td>0.005096</td>\n      <td>1.240280</td>\n      <td>-1.078799</td>\n      <td>0.317837</td>\n      <td>0.626911</td>\n      <td>0.032810</td>\n      <td>-0.529699</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>0.559587</td>\n      <td>0.877735</td>\n      <td>1.646569</td>\n      <td>1.005932</td>\n      <td>0.093156</td>\n      <td>-1.212238</td>\n      <td>0.125245</td>\n      <td>0.202628</td>\n      <td>0.747672</td>\n      <td>1.313196</td>\n      <td>...</td>\n      <td>-0.453492</td>\n      <td>0.024436</td>\n      <td>-1.320878</td>\n      <td>1.907564</td>\n      <td>0.905234</td>\n      <td>-0.718263</td>\n      <td>-1.216225</td>\n      <td>-1.042418</td>\n      <td>0.642308</td>\n      <td>-1.049911</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>-1.515261</td>\n      <td>0.175513</td>\n      <td>0.699545</td>\n      <td>-1.373012</td>\n      <td>-0.187130</td>\n      <td>0.797151</td>\n      <td>-2.858676</td>\n      <td>1.277270</td>\n      <td>0.391446</td>\n      <td>0.675390</td>\n      <td>...</td>\n      <td>-0.763045</td>\n      <td>-1.129448</td>\n      <td>0.636055</td>\n      <td>0.906020</td>\n      <td>1.198538</td>\n      <td>-0.257622</td>\n      <td>0.434496</td>\n      <td>0.940782</td>\n      <td>-1.300555</td>\n      <td>0.258037</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-1.523851</td>\n      <td>-0.267471</td>\n      <td>0.285857</td>\n      <td>1.783188</td>\n      <td>0.447981</td>\n      <td>0.707124</td>\n      <td>1.099610</td>\n      <td>-1.013814</td>\n      <td>-0.128593</td>\n      <td>2.385796</td>\n      <td>...</td>\n      <td>-1.269593</td>\n      <td>1.183064</td>\n      <td>-0.421932</td>\n      <td>0.058725</td>\n      <td>-0.629062</td>\n      <td>-0.277465</td>\n      <td>0.184457</td>\n      <td>0.814573</td>\n      <td>0.957620</td>\n      <td>0.415154</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 100 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nX = data_re.values\ny = data['y'].values\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)  \n\n# 创建DataLoader\ndataset = TensorDataset(X_tensor, y_tensor)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:26:40.619066Z","iopub.execute_input":"2024-06-24T03:26:40.619486Z","iopub.status.idle":"2024-06-24T03:26:40.665711Z","shell.execute_reply.started":"2024-06-24T03:26:40.619453Z","shell.execute_reply":"2024-06-24T03:26:40.664267Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class SimpleMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x, _=None):  \n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# defind model\ninput_dim = X.shape[1]\nhidden_dim = 264\noutput_dim = len(set(y))  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:26:40.667068Z","iopub.execute_input":"2024-06-24T03:26:40.667496Z","iopub.status.idle":"2024-06-24T03:26:40.700410Z","shell.execute_reply.started":"2024-06-24T03:26:40.667464Z","shell.execute_reply":"2024-06-24T03:26:40.698953Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 50\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:26:40.703660Z","iopub.execute_input":"2024-06-24T03:26:40.704171Z","iopub.status.idle":"2024-06-24T03:26:43.997957Z","shell.execute_reply.started":"2024-06-24T03:26:40.704110Z","shell.execute_reply":"2024-06-24T03:26:43.996756Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def add_dimension_glasso(var, dim=0):\n    return var.pow(2).sum(dim=dim).add(1e-8).pow(1/2.).sum()\n\ndef deep_lasso_regularizer(loss, inputs):\n    grad_params = autograd.grad(loss, inputs, create_graph=True, allow_unused=True)\n    regval = add_dimension_glasso(grad_params[0], dim=0)\n    return regval\n\ndef get_feat_importance_deeplasso(net, testloader, criterion, device):\n    net.eval()\n    grads = []\n    for batch_idx, (inputs_num, targets) in enumerate(testloader):\n        inputs_num, targets = inputs_num.to(device), targets.to(device)\n        inputs_num.requires_grad_()\n        outputs = net(inputs_num)\n        loss = criterion(outputs, targets)\n        reg_loss = deep_lasso_regularizer(loss, inputs_num)  # 使用 deep_lasso_regularizer 函数\n        grad_params = autograd.grad(reg_loss, inputs_num, create_graph=True, allow_unused=True)\n        grads.append(grad_params[0].detach().cpu())\n\n    grads = torch.cat(grads)\n    importances = grads.pow(2).sum(dim=0).pow(1/2.)\n    return importances\n\ntest_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n\n# calculate features importance\nimportances = get_feat_importance_deeplasso(model, test_loader, criterion, device)\n\nfeature_names = data_re.columns\nimportance_indices = torch.argsort(importances, descending=True)\ntop_33_indices = importance_indices[:33]\ntop_33_features = feature_names[top_33_indices]\n\nprint(\"Top 33 Important Features:\")\nfor i, feature in enumerate(top_33_features):\n    print(f\"{i+1}: {feature} - Importance: {importances[top_33_indices[i]].item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:26:43.999504Z","iopub.execute_input":"2024-06-24T03:26:44.000024Z","iopub.status.idle":"2024-06-24T03:26:44.072585Z","shell.execute_reply.started":"2024-06-24T03:26:43.999991Z","shell.execute_reply":"2024-06-24T03:26:44.071270Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Top 33 Important Features:\n1: var27 - Importance: 0.05092018470168114\n2: var23 - Importance: 0.04833841323852539\n3: var10 - Importance: 0.04515768587589264\n4: var26 - Importance: 0.04285818338394165\n5: var6 - Importance: 0.04181760549545288\n6: var16 - Importance: 0.03413280099630356\n7: var31 - Importance: 0.0333033986389637\n8: var24 - Importance: 0.0319250151515007\n9: var1 - Importance: 0.03162641450762749\n10: var30 - Importance: 0.028380122035741806\n11: var25 - Importance: 0.02618580311536789\n12: var7 - Importance: 0.02521054819226265\n13: var22 - Importance: 0.022943640127778053\n14: var13 - Importance: 0.021216843277215958\n15: var71 - Importance: 0.018839480355381966\n16: var20 - Importance: 0.017960915341973305\n17: var11 - Importance: 0.017819978296756744\n18: var19 - Importance: 0.017170941457152367\n19: var32 - Importance: 0.016565166413784027\n20: var0 - Importance: 0.014993754215538502\n21: var29 - Importance: 0.014234738424420357\n22: var50 - Importance: 0.013746044598519802\n23: var28 - Importance: 0.01341306697577238\n24: var14 - Importance: 0.013269556686282158\n25: var85 - Importance: 0.012763039208948612\n26: var54 - Importance: 0.0119957709684968\n27: var78 - Importance: 0.011745764873921871\n28: var83 - Importance: 0.011511998251080513\n29: var91 - Importance: 0.011223570443689823\n30: var98 - Importance: 0.011217795312404633\n31: var55 - Importance: 0.011051574721932411\n32: var38 - Importance: 0.010974755510687828\n33: var2 - Importance: 0.010694303549826145\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"def normal_prediction(X_train,y_train,X_test,y_test,input_s):\n    input_dim = input_s\n    running_times = []\n    acc= []\n    for i in range(0,10,1):\n        model = keras.Sequential()\n        model.add(layers.Dense(10, input_shape=(input_dim,)))\n\n        model.add(layers.Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        start_time = time.time()\n        model.fit(X_train,y_train, epochs=250, batch_size=32, verbose=0)\n        predictions = model.predict(X_test)\n        predicted = (predictions > 0.5)  \n        accuracy = accuracy_score(y_test, predicted) \n       \n        end_time = time.time()\n        run_time = end_time - start_time\n        running_times.append(run_time)\n        acc.append(accuracy)\n    avg_running_time = np.mean(running_times)\n    avg_accuracy = np.mean(acc)\n\n    print(\"Average running time:\", avg_running_time, \"s\")\n    print(\"Average accuracy:\", avg_accuracy * 100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:52:51.829405Z","iopub.execute_input":"2024-06-24T03:52:51.829834Z","iopub.status.idle":"2024-06-24T03:52:51.841382Z","shell.execute_reply.started":"2024-06-24T03:52:51.829800Z","shell.execute_reply":"2024-06-24T03:52:51.839740Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nvar_t = data.loc[:, ['var27',\n'var23',\n'var10',\n'var26',\n'var6',\n'var16',\n'var31',\n'var24',\n'var1',\n'var30',\n'var25',\n'var7',\n'var22',\n'var13',\n'var71',\n'var20',\n'var11',\n'var19',\n'var32',\n'var0','y']]\n\nX_train, X_test, y_train, y_test = train_test_split(var_t.drop(columns=['y']), var_t['y'], test_size=0.2, random_state=42)\ninput_data = []\noutput_data = []\ninput_test =  []\noutput_test = []\nfor i in X_train.values:\n    input_data.append(i)\nfor i in y_train.values:  \n    output_data.append(i)\nfor i in X_test.values:\n    input_test.append(i)\nfor i in y_test.values:\n    output_test.append(i)\n    \nnormal_prediction(X_train,y_train,X_test,y_test,np.shape(var_t)[1]-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:54:12.738030Z","iopub.execute_input":"2024-06-24T03:54:12.738792Z","iopub.status.idle":"2024-06-24T03:56:10.409233Z","shell.execute_reply.started":"2024-06-24T03:54:12.738754Z","shell.execute_reply":"2024-06-24T03:56:10.407947Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\nAverage running time: 11.738960337638854 s\nAverage accuracy: 82.0 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Application data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndf = pd.read_excel('/kaggle/input/applicationdata/cancer.xlsx')\ndata = df.rename(columns={'Column1': 'ID', 'Column2': 'Y', 'Column3': 'radius1', 'Column4': 'texture1', 'Column5': 'perimeter1', \n                     'Column6': 'area1', 'Column7': 'smoothness1', 'Column8': 'compactness1', 'Column9': 'concavity1', 'Column10': 'concave_points1',\n                     'Column11': 'symmetry1', 'Column12': 'fractal_dimension1', 'Column13': 'radius2', 'Column14': 'texture2', 'Column15': 'perimeter2', \n                     'Column16': 'area2', 'Column17': 'smoothness2', 'Column18': 'compactness2', 'Column19': 'concavity2', 'Column20': 'concave_points2',\n                     'Column21': 'symmetry2', 'Column22': 'fractal_dimension2', 'Column23': 'radius3', 'Column24': 'texture3', 'Column25': 'perimeter3', \n                     'Column26': 'area3', 'Column27':'smoothness3', 'Column28': 'compactness3', 'Column29': 'concavity3', 'Column30': 'concave_points3',\n                     'Column31': 'symmetry3', 'Column32': 'fractal_dimension3'})\ndata['Y'] = data['Y'].replace({'B': 0, 'M': 1})\n# features_to_normalize = ['var1', 'var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var0']\ncols_to_normalize = data.columns[2:]\n\n# Normalize the selected columns using Min-Max scaling\nscaler = MinMaxScaler()\ndata[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:06.849212Z","iopub.execute_input":"2024-06-24T03:35:06.849647Z","iopub.status.idle":"2024-06-24T03:35:07.305367Z","shell.execute_reply.started":"2024-06-24T03:35:06.849614Z","shell.execute_reply":"2024-06-24T03:35:07.304131Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"           ID  Y   radius1  texture1  perimeter1     area1  smoothness1  \\\n0      842302  1  0.521037  0.022658    0.545989  0.363733     0.593753   \n1      842517  1  0.643144  0.272574    0.615783  0.501591     0.289880   \n2    84300903  1  0.601496  0.390260    0.595743  0.449417     0.514309   \n3    84348301  1  0.210090  0.360839    0.233501  0.102906     0.811321   \n4    84358402  1  0.629893  0.156578    0.630986  0.489290     0.430351   \n..        ... ..       ...       ...         ...       ...          ...   \n564    926424  1  0.690000  0.428813    0.678668  0.566490     0.526948   \n565    926682  1  0.622320  0.626987    0.604036  0.474019     0.407782   \n566    926954  1  0.455251  0.621238    0.445788  0.303118     0.288165   \n567    927241  1  0.644564  0.663510    0.665538  0.475716     0.588336   \n568     92751  0  0.036869  0.501522    0.028540  0.015907     0.000000   \n\n     compactness1  concavity1  concave_points1  ...   radius3  texture3  \\\n0        0.792037    0.703140         0.731113  ...  0.620776  0.141525   \n1        0.181768    0.203608         0.348757  ...  0.606901  0.303571   \n2        0.431017    0.462512         0.635686  ...  0.556386  0.360075   \n3        0.811361    0.565604         0.522863  ...  0.248310  0.385928   \n4        0.347893    0.463918         0.518390  ...  0.519744  0.123934   \n..            ...         ...              ...  ...       ...       ...   \n564      0.296055    0.571462         0.690358  ...  0.623266  0.383262   \n565      0.257714    0.337395         0.486630  ...  0.560655  0.699094   \n566      0.254340    0.216753         0.263519  ...  0.393099  0.589019   \n567      0.790197    0.823336         0.755467  ...  0.633582  0.730277   \n568      0.074351    0.000000         0.000000  ...  0.054287  0.489072   \n\n     perimeter3     area3  smoothness3  compactness3  concavity3  \\\n0      0.668310  0.450698     0.601136      0.619292    0.568610   \n1      0.539818  0.435214     0.347553      0.154563    0.192971   \n2      0.508442  0.374508     0.483590      0.385375    0.359744   \n3      0.241347  0.094008     0.915472      0.814012    0.548642   \n4      0.506948  0.341575     0.437364      0.172415    0.319489   \n..          ...       ...          ...           ...         ...   \n564    0.576174  0.452664     0.461137      0.178527    0.328035   \n565    0.520892  0.379915     0.300007      0.159997    0.256789   \n566    0.379949  0.230731     0.282177      0.273705    0.271805   \n567    0.668310  0.402035     0.619626      0.815758    0.749760   \n568    0.043578  0.020497     0.124084      0.036043    0.000000   \n\n     concave_points3  symmetry3  fractal_dimension3  \n0           0.912027   0.598462            0.418864  \n1           0.639175   0.233590            0.222878  \n2           0.835052   0.403706            0.213433  \n3           0.884880   1.000000            0.773711  \n4           0.558419   0.157500            0.142595  \n..               ...        ...                 ...  \n564         0.761512   0.097575            0.105667  \n565         0.559450   0.198502            0.074315  \n566         0.487285   0.128721            0.151909  \n567         0.910653   0.497142            0.452315  \n568         0.000000   0.257441            0.100682  \n\n[569 rows x 32 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Y</th>\n      <th>radius1</th>\n      <th>texture1</th>\n      <th>perimeter1</th>\n      <th>area1</th>\n      <th>smoothness1</th>\n      <th>compactness1</th>\n      <th>concavity1</th>\n      <th>concave_points1</th>\n      <th>...</th>\n      <th>radius3</th>\n      <th>texture3</th>\n      <th>perimeter3</th>\n      <th>area3</th>\n      <th>smoothness3</th>\n      <th>compactness3</th>\n      <th>concavity3</th>\n      <th>concave_points3</th>\n      <th>symmetry3</th>\n      <th>fractal_dimension3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>1</td>\n      <td>0.521037</td>\n      <td>0.022658</td>\n      <td>0.545989</td>\n      <td>0.363733</td>\n      <td>0.593753</td>\n      <td>0.792037</td>\n      <td>0.703140</td>\n      <td>0.731113</td>\n      <td>...</td>\n      <td>0.620776</td>\n      <td>0.141525</td>\n      <td>0.668310</td>\n      <td>0.450698</td>\n      <td>0.601136</td>\n      <td>0.619292</td>\n      <td>0.568610</td>\n      <td>0.912027</td>\n      <td>0.598462</td>\n      <td>0.418864</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>1</td>\n      <td>0.643144</td>\n      <td>0.272574</td>\n      <td>0.615783</td>\n      <td>0.501591</td>\n      <td>0.289880</td>\n      <td>0.181768</td>\n      <td>0.203608</td>\n      <td>0.348757</td>\n      <td>...</td>\n      <td>0.606901</td>\n      <td>0.303571</td>\n      <td>0.539818</td>\n      <td>0.435214</td>\n      <td>0.347553</td>\n      <td>0.154563</td>\n      <td>0.192971</td>\n      <td>0.639175</td>\n      <td>0.233590</td>\n      <td>0.222878</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>1</td>\n      <td>0.601496</td>\n      <td>0.390260</td>\n      <td>0.595743</td>\n      <td>0.449417</td>\n      <td>0.514309</td>\n      <td>0.431017</td>\n      <td>0.462512</td>\n      <td>0.635686</td>\n      <td>...</td>\n      <td>0.556386</td>\n      <td>0.360075</td>\n      <td>0.508442</td>\n      <td>0.374508</td>\n      <td>0.483590</td>\n      <td>0.385375</td>\n      <td>0.359744</td>\n      <td>0.835052</td>\n      <td>0.403706</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>1</td>\n      <td>0.210090</td>\n      <td>0.360839</td>\n      <td>0.233501</td>\n      <td>0.102906</td>\n      <td>0.811321</td>\n      <td>0.811361</td>\n      <td>0.565604</td>\n      <td>0.522863</td>\n      <td>...</td>\n      <td>0.248310</td>\n      <td>0.385928</td>\n      <td>0.241347</td>\n      <td>0.094008</td>\n      <td>0.915472</td>\n      <td>0.814012</td>\n      <td>0.548642</td>\n      <td>0.884880</td>\n      <td>1.000000</td>\n      <td>0.773711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>1</td>\n      <td>0.629893</td>\n      <td>0.156578</td>\n      <td>0.630986</td>\n      <td>0.489290</td>\n      <td>0.430351</td>\n      <td>0.347893</td>\n      <td>0.463918</td>\n      <td>0.518390</td>\n      <td>...</td>\n      <td>0.519744</td>\n      <td>0.123934</td>\n      <td>0.506948</td>\n      <td>0.341575</td>\n      <td>0.437364</td>\n      <td>0.172415</td>\n      <td>0.319489</td>\n      <td>0.558419</td>\n      <td>0.157500</td>\n      <td>0.142595</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>926424</td>\n      <td>1</td>\n      <td>0.690000</td>\n      <td>0.428813</td>\n      <td>0.678668</td>\n      <td>0.566490</td>\n      <td>0.526948</td>\n      <td>0.296055</td>\n      <td>0.571462</td>\n      <td>0.690358</td>\n      <td>...</td>\n      <td>0.623266</td>\n      <td>0.383262</td>\n      <td>0.576174</td>\n      <td>0.452664</td>\n      <td>0.461137</td>\n      <td>0.178527</td>\n      <td>0.328035</td>\n      <td>0.761512</td>\n      <td>0.097575</td>\n      <td>0.105667</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>926682</td>\n      <td>1</td>\n      <td>0.622320</td>\n      <td>0.626987</td>\n      <td>0.604036</td>\n      <td>0.474019</td>\n      <td>0.407782</td>\n      <td>0.257714</td>\n      <td>0.337395</td>\n      <td>0.486630</td>\n      <td>...</td>\n      <td>0.560655</td>\n      <td>0.699094</td>\n      <td>0.520892</td>\n      <td>0.379915</td>\n      <td>0.300007</td>\n      <td>0.159997</td>\n      <td>0.256789</td>\n      <td>0.559450</td>\n      <td>0.198502</td>\n      <td>0.074315</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>926954</td>\n      <td>1</td>\n      <td>0.455251</td>\n      <td>0.621238</td>\n      <td>0.445788</td>\n      <td>0.303118</td>\n      <td>0.288165</td>\n      <td>0.254340</td>\n      <td>0.216753</td>\n      <td>0.263519</td>\n      <td>...</td>\n      <td>0.393099</td>\n      <td>0.589019</td>\n      <td>0.379949</td>\n      <td>0.230731</td>\n      <td>0.282177</td>\n      <td>0.273705</td>\n      <td>0.271805</td>\n      <td>0.487285</td>\n      <td>0.128721</td>\n      <td>0.151909</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>927241</td>\n      <td>1</td>\n      <td>0.644564</td>\n      <td>0.663510</td>\n      <td>0.665538</td>\n      <td>0.475716</td>\n      <td>0.588336</td>\n      <td>0.790197</td>\n      <td>0.823336</td>\n      <td>0.755467</td>\n      <td>...</td>\n      <td>0.633582</td>\n      <td>0.730277</td>\n      <td>0.668310</td>\n      <td>0.402035</td>\n      <td>0.619626</td>\n      <td>0.815758</td>\n      <td>0.749760</td>\n      <td>0.910653</td>\n      <td>0.497142</td>\n      <td>0.452315</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>92751</td>\n      <td>0</td>\n      <td>0.036869</td>\n      <td>0.501522</td>\n      <td>0.028540</td>\n      <td>0.015907</td>\n      <td>0.000000</td>\n      <td>0.074351</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.054287</td>\n      <td>0.489072</td>\n      <td>0.043578</td>\n      <td>0.020497</td>\n      <td>0.124084</td>\n      <td>0.036043</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.257441</td>\n      <td>0.100682</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 32 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = data.drop(columns=['Y']).values\ny = data['Y'].values\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)  \n\ndataset = TensorDataset(X_tensor, y_tensor)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:07.307857Z","iopub.execute_input":"2024-06-24T03:35:07.308322Z","iopub.status.idle":"2024-06-24T03:35:07.320268Z","shell.execute_reply.started":"2024-06-24T03:35:07.308284Z","shell.execute_reply":"2024-06-24T03:35:07.319003Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"input_dim = X.shape[1]\nhidden_dim = 10\noutput_dim = len(set(y))\n\nmodel = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:07.322057Z","iopub.execute_input":"2024-06-24T03:35:07.322621Z","iopub.status.idle":"2024-06-24T03:35:07.332870Z","shell.execute_reply.started":"2024-06-24T03:35:07.322583Z","shell.execute_reply":"2024-06-24T03:35:07.331465Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:07.334721Z","iopub.execute_input":"2024-06-24T03:35:07.335204Z","iopub.status.idle":"2024-06-24T03:35:08.248997Z","shell.execute_reply.started":"2024-06-24T03:35:07.335140Z","shell.execute_reply":"2024-06-24T03:35:08.247820Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n\nimportances = get_feat_importance_deeplasso(model, test_loader, criterion, device)\n\nfeature_names = data.drop(columns=['Y']).columns\n\nimportance_indices = torch.argsort(importances, descending=True)\n\ntop_33_indices = importance_indices[:33]\ntop_33_features = feature_names[top_33_indices]\n\nprint(\"Top 33 Important Features:\")\nfor i, feature in enumerate(top_33_features):\n    print(f\"{i+1}: {feature} - Importance: {importances[top_33_indices[i]].item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:08.251787Z","iopub.execute_input":"2024-06-24T03:35:08.252754Z","iopub.status.idle":"2024-06-24T03:35:08.291363Z","shell.execute_reply.started":"2024-06-24T03:35:08.252717Z","shell.execute_reply":"2024-06-24T03:35:08.290193Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Top 33 Important Features:\n1: smoothness2 - Importance: 0.00012321244867052883\n2: concavity2 - Importance: 0.00011928225285373628\n3: fractal_dimension3 - Importance: 0.00010587593715172261\n4: radius1 - Importance: 0.0001035825043800287\n5: concavity1 - Importance: 0.00010283744632033631\n6: area1 - Importance: 0.00010024413495557383\n7: texture1 - Importance: 9.941057214746252e-05\n8: symmetry3 - Importance: 9.011509246192873e-05\n9: fractal_dimension1 - Importance: 8.62941742525436e-05\n10: smoothness1 - Importance: 7.579434895887971e-05\n11: concavity3 - Importance: 6.92301255185157e-05\n12: concave_points1 - Importance: 4.51921732746996e-05\n13: compactness3 - Importance: 4.245576565153897e-05\n14: compactness1 - Importance: 4.208533209748566e-05\n15: radius3 - Importance: 4.076876939507201e-05\n16: area3 - Importance: 3.6814417399000376e-05\n17: concave_points3 - Importance: 3.524822386680171e-05\n18: symmetry2 - Importance: 3.2606661989120767e-05\n19: radius2 - Importance: 2.9302635084604844e-05\n20: symmetry1 - Importance: 2.5751589419087395e-05\n21: perimeter2 - Importance: 2.3657721612835303e-05\n22: compactness2 - Importance: 2.3113070710678585e-05\n23: smoothness3 - Importance: 1.7065307474695146e-05\n24: perimeter1 - Importance: 1.5181970411504153e-05\n25: texture3 - Importance: 1.4408996321435552e-05\n26: concave_points2 - Importance: 1.186954523291206e-05\n27: area2 - Importance: 1.1499464562803041e-05\n28: fractal_dimension2 - Importance: 1.006567708827788e-05\n29: perimeter3 - Importance: 6.910258434800198e-06\n30: texture2 - Importance: 6.734112957929028e-06\n31: ID - Importance: 2.605768543162412e-07\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"def normal_prediction(X_train,y_train,X_test,y_test,input_s):\n    input_dim = input_s\n    running_times = []\n    acc= []\n    for i in range(0,10,1):\n        model = keras.Sequential()\n        model.add(layers.Dense(10, input_shape=(input_dim,)))\n\n        model.add(layers.Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n        start_time = time.time()\n        model.fit(X_train,y_train, epochs=250, batch_size=32, verbose=0)\n        predictions = model.predict(X_test)\n        predicted = (predictions > 0.5)  \n        accuracy = accuracy_score(y_test, predicted) \n       \n        end_time = time.time()\n        run_time = end_time - start_time\n        running_times.append(run_time)\n        acc.append(accuracy)\n    avg_running_time = np.mean(running_times)\n    avg_accuracy = np.mean(acc)\n\n    print(\"Average running time:\", avg_running_time, \"s\")\n    print(\"Average accuracy:\", avg_accuracy * 100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:42:39.236001Z","iopub.execute_input":"2024-06-24T03:42:39.236882Z","iopub.status.idle":"2024-06-24T03:42:39.250822Z","shell.execute_reply.started":"2024-06-24T03:42:39.236845Z","shell.execute_reply":"2024-06-24T03:42:39.249353Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"var_t = data.loc[:, ['Y',\n'smoothness2', \n'concavity2',\n'fractal_dimension3',\n'radius1',\n'concavity1',\n'area1',\n'texture1',\n'symmetry3',\n'fractal_dimension1',\n'smoothness1',\n'concavity3',\n'concave_points1']]\n\nX_train, X_test, y_train, y_test = train_test_split(var_t.drop(columns=['Y']), var_t['Y'], test_size=0.2, random_state=42)\ninput_data = []\noutput_data = []\ninput_test =  []\noutput_test = []\nfor i in X_train.values:\n    input_data.append(i)\nfor i in y_train.values:  \n    output_data.append(i)\nfor i in X_test.values:\n    input_test.append(i)\nfor i in y_test.values:\n    output_test.append(i)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:46:42.520913Z","iopub.execute_input":"2024-06-24T03:46:42.521357Z","iopub.status.idle":"2024-06-24T03:46:42.538083Z","shell.execute_reply.started":"2024-06-24T03:46:42.521322Z","shell.execute_reply":"2024-06-24T03:46:42.536794Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"normal_prediction(X_train,y_train,X_test,y_test,np.shape(var_t)[1]-1)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:48:39.713928Z","iopub.execute_input":"2024-06-24T03:48:39.714370Z","iopub.status.idle":"2024-06-24T03:50:46.446311Z","shell.execute_reply.started":"2024-06-24T03:48:39.714336Z","shell.execute_reply":"2024-06-24T03:50:46.445010Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\nAverage running time: 12.646170353889465 s\nAverage accuracy: 95.6140350877193 %\n","output_type":"stream"}]}]}