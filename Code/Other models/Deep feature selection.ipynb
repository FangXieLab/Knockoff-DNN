{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#User-Guide-on-nonlinear-example\" data-toc-modified-id=\"User-Guide-on-nonlinear-example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>User Guide on nonlinear example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#DFS-with-fixed-hyper-parameters\" data-toc-modified-id=\"DFS-with-fixed-hyper-parameters-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>DFS with fixed hyper-parameters</a></span></li><li><span><a href=\"#Selection-of-$s$\" data-toc-modified-id=\"Selection-of-$s$-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Selection of $s$</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Selection\n",
    "In this notebook, we will demonstrate how to implement our method on the nonlinear simulation examples from our paper.\n",
    "## User Guide on nonlinear example\n",
    "In this example, a high dimensional dataset with 500 covariates and 300 observations is generated using the following equation\n",
    "\n",
    "\\begin{equation}\n",
    "    y=\\begin{cases}\n",
    "        1, & e^{x_1} + x_2^2 + 5\\sin(x_3 x_4) - 3 > 0\\\\\n",
    "        0, & \\text{otherwise,}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "i.e. among 500 covariates, only the first 4 variables actually contributed to the response. Our task is to correctly select the important variables. Please see section 5.2 of the paper for detailed generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../src\")\n",
    "#from time import clock\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "from torch.nn.parameter import Parameter\n",
    "from utils import data_load_n, data_load_l, measure, accuracy\n",
    "from models import Net_nonlinear, Net_linear\n",
    "from dfs import DFS_epoch, training_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We will load our data in the following chunk. The data, both covariates and response, need to be load as `pytorch` `Tensor` objects to be fed into the DFS algorithm. The function `data_load_n` will read in dataset and split it into training and test set so that both sets have same number of positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       var0      var1      var2      var3      var4      var5      var6  \\\n",
      "0  0.496714 -0.138264  0.647689  1.523030 -0.234153 -0.234137  1.579213   \n",
      "1 -1.057711  0.822545 -1.220844  0.208864 -1.959670 -1.328186  0.196861   \n",
      "2 -0.072010  1.003533  0.361636 -0.645120  0.361396  1.538037 -0.035826   \n",
      "3 -0.234587 -1.415371 -0.420645 -0.342715 -0.802277 -0.161286  0.404051   \n",
      "4 -1.062304  0.473592 -0.919424  1.549934 -0.783253 -0.322062  0.813517   \n",
      "\n",
      "       var7      var8      var9  ...     var91     var92     var93     var94  \\\n",
      "0  0.767435 -0.469474  0.542560  ... -0.029352  0.395307  0.033023  1.346941   \n",
      "1  0.738467  0.171368 -0.115648  ...  1.021963  0.733179  1.378143 -0.990623   \n",
      "2  1.564644 -2.619745  0.821903  ...  0.272634  0.342226 -1.098679  0.044570   \n",
      "3  1.886186  0.174578  0.257550  ... -1.312467  0.536389 -1.671147 -0.838362   \n",
      "4 -1.230864  0.227460  1.307143  ...  0.534347 -1.768415  0.995168  0.937367   \n",
      "\n",
      "      var95     var96     var97     var98     var99  y  \n",
      "0  0.774023 -0.007481  0.216689  0.295666  0.403300  1  \n",
      "1 -0.343192  0.758982  0.448078  1.532380  0.420537  0  \n",
      "2  0.631224 -1.151227  1.104997 -0.377531  1.257613  1  \n",
      "3 -1.212657  0.781943 -0.065410 -0.013503 -1.106733  0  \n",
      "4  0.830161  0.372842  0.220189  1.032389  1.923513  0  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.normal(loc=0, scale=1, size=(input_size, hidden_size))\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.normal(loc=0, scale=1, size=(hidden_size, output_size))\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    Z1 = np.dot(X, parameters['W1']) + parameters['b1']\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, parameters['W2']) + parameters['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    return {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "\n",
    "def predict(X, parameters):\n",
    "    forward = forward_propagation(X, parameters)\n",
    "    return (forward['A2'] > 0.5).astype(int)\n",
    "\n",
    "# Set up neural network parameters\n",
    "input_size = 100\n",
    "hidden_size = 264\n",
    "output_size = 1\n",
    "\n",
    "# Generate random input data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate the first 33 variables using the neural network\n",
    "X_first_33 = np.random.normal(loc=0, scale=1, size=(n_samples, 33))\n",
    "parameters = initialize_parameters(33, hidden_size, output_size)\n",
    "forward = forward_propagation(X_first_33, parameters)\n",
    "y_neural_network = forward['A2']\n",
    "\n",
    "# Generate random values for the remaining variables\n",
    "X_rest = np.random.normal(loc=0, scale=1, size=(n_samples, input_size - 33))\n",
    "\n",
    "# Combine the generated values\n",
    "X = np.concatenate((X_first_33, X_rest), axis=1)\n",
    "\n",
    "# Threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_binary_neural_network = (y_neural_network > threshold).astype(int)\n",
    "\n",
    "# Add the generated y values to the existing DataFrame\n",
    "data = pd.DataFrame(X, columns=[f'var{i}' for i in range(input_size)])\n",
    "data['y'] = y_binary_neural_network\n",
    "\n",
    "# Print the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:,:-1]\n",
    "X = (X - X.mean()) / X.std()\n",
    "Y = data.iloc[:,-1]\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Y</th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Y  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
       "0      842302  1    17.99     10.38      122.80  1001.0      0.11840   \n",
       "1      842517  1    20.57     17.77      132.90  1326.0      0.08474   \n",
       "2    84300903  1    19.69     21.25      130.00  1203.0      0.10960   \n",
       "3    84348301  1    11.42     20.38       77.58   386.1      0.14250   \n",
       "4    84358402  1    20.29     14.34      135.10  1297.0      0.10030   \n",
       "..        ... ..      ...       ...         ...     ...          ...   \n",
       "564    926424  1    21.56     22.39      142.00  1479.0      0.11100   \n",
       "565    926682  1    20.13     28.25      131.20  1261.0      0.09780   \n",
       "566    926954  1    16.60     28.08      108.30   858.1      0.08455   \n",
       "567    927241  1    20.60     29.33      140.10  1265.0      0.11780   \n",
       "568     92751  0     7.76     24.54       47.92   181.0      0.05263   \n",
       "\n",
       "     compactness1  concavity1  concave_points1  ...  radius3  texture3  \\\n",
       "0         0.27760     0.30010          0.14710  ...   25.380     17.33   \n",
       "1         0.07864     0.08690          0.07017  ...   24.990     23.41   \n",
       "2         0.15990     0.19740          0.12790  ...   23.570     25.53   \n",
       "3         0.28390     0.24140          0.10520  ...   14.910     26.50   \n",
       "4         0.13280     0.19800          0.10430  ...   22.540     16.67   \n",
       "..            ...         ...              ...  ...      ...       ...   \n",
       "564       0.11590     0.24390          0.13890  ...   25.450     26.40   \n",
       "565       0.10340     0.14400          0.09791  ...   23.690     38.25   \n",
       "566       0.10230     0.09251          0.05302  ...   18.980     34.12   \n",
       "567       0.27700     0.35140          0.15200  ...   25.740     39.42   \n",
       "568       0.04362     0.00000          0.00000  ...    9.456     30.37   \n",
       "\n",
       "     perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0        184.60  2019.0      0.16220       0.66560      0.7119   \n",
       "1        158.80  1956.0      0.12380       0.18660      0.2416   \n",
       "2        152.50  1709.0      0.14440       0.42450      0.4504   \n",
       "3         98.87   567.7      0.20980       0.86630      0.6869   \n",
       "4        152.20  1575.0      0.13740       0.20500      0.4000   \n",
       "..          ...     ...          ...           ...         ...   \n",
       "564      166.10  2027.0      0.14100       0.21130      0.4107   \n",
       "565      155.00  1731.0      0.11660       0.19220      0.3215   \n",
       "566      126.70  1124.0      0.11390       0.30940      0.3403   \n",
       "567      184.60  1821.0      0.16500       0.86810      0.9387   \n",
       "568       59.16   268.6      0.08996       0.06444      0.0000   \n",
       "\n",
       "     concave_points3  symmetry3  fractal_dimension3  \n",
       "0             0.2654     0.4601             0.11890  \n",
       "1             0.1860     0.2750             0.08902  \n",
       "2             0.2430     0.3613             0.08758  \n",
       "3             0.2575     0.6638             0.17300  \n",
       "4             0.1625     0.2364             0.07678  \n",
       "..               ...        ...                 ...  \n",
       "564           0.2216     0.2060             0.07115  \n",
       "565           0.1628     0.2572             0.06637  \n",
       "566           0.1418     0.2218             0.07820  \n",
       "567           0.2650     0.4087             0.12400  \n",
       "568           0.0000     0.2871             0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = pd.read_excel('cancer.xlsx')\n",
    "data = df.rename(columns={'Column1': 'ID', 'Column2': 'Y', 'Column3': 'radius1', 'Column4': 'texture1', 'Column5': 'perimeter1', \n",
    "                     'Column6': 'area1', 'Column7': 'smoothness1', 'Column8': 'compactness1', 'Column9': 'concavity1', 'Column10': 'concave_points1',\n",
    "                     'Column11': 'symmetry1', 'Column12': 'fractal_dimension1', 'Column13': 'radius2', 'Column14': 'texture2', 'Column15': 'perimeter2', \n",
    "                     'Column16': 'area2', 'Column17': 'smoothness2', 'Column18': 'compactness2', 'Column19': 'concavity2', 'Column20': 'concave_points2',\n",
    "                     'Column21': 'symmetry2', 'Column22': 'fractal_dimension2', 'Column23': 'radius3', 'Column24': 'texture3', 'Column25': 'perimeter3', \n",
    "                     'Column26': 'area3', 'Column27':'smoothness3', 'Column28': 'compactness3', 'Column29': 'concavity3', 'Column30': 'concave_points3',\n",
    "                     'Column31': 'symmetry3', 'Column32': 'fractal_dimension3'})\n",
    "data['Y'] = data['Y'].replace({'B': 0, 'M': 1})\n",
    "# features_to_normalize = ['var1', 'var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var0']\n",
    "cols_to_normalize = data.columns[2:]\n",
    "\n",
    "# Normalize the selected columns using Min-Max scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(columns=['Y'])\n",
    "X = (X - X.mean()) / X.std()\n",
    "Y = data.iloc[:,-1]\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFS with fixed hyper-parameters\n",
    "In this section, we demonstrate how to run DFS with one given set of hyper-parameters. The hyper-parameters includes:\n",
    "* `s`, the number of variables to be selected;\n",
    "* `c`, the tunning parameters to control the magnitude of $\\lambda_1$ and $\\lambda_2$;\n",
    "* `epochs`, the number of DFS iterations to be run;\n",
    "* `n_hidden1` & `n_hidden2`, the number of neurons in the fully connect neural networks;\n",
    "* `learning_rate`, the learning rate for optimizer;\n",
    "* `Ts` & `step`, the parameters to control the optimization on given support\n",
    "\n",
    "Among the above hyper-parameters, `s` is the most important parameters, and the selection of $s$ will be demonstrated in next sections. `c` can be selection through a sequence of candidates that returns the smallest loss function. Others mostly are meant to help the convergence of the optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 2 epochs\n"
     ]
    }
   ],
   "source": [
    "# specify hyper-paramters\n",
    "import torch\n",
    "N, p = X.shape\n",
    "s = 2\n",
    "c = 1\n",
    "epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 10\n",
    "learning_rate = 0.05\n",
    "Ts = 25 # To avoid long time waiting, this parameter has been shorten\n",
    "step = 5\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y = torch.tensor(Y.values, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "# Define Model\n",
    "torch.manual_seed(1) # set seed\n",
    "# Define a model with pre-specified structure and hyper parameters\n",
    "model = Net_nonlinear(n_feature=p, n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_output=2)\n",
    "# Define another model to save the current best model based on loss function value\n",
    "# The purpose is to prevent divergence of the training due to large learning rate or other reason\n",
    "best_model = Net_nonlinear(n_feature=p, n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_output=2)\n",
    "\n",
    "\n",
    "# Define optimizers for the optimization with given support\n",
    "# optimizer to separately optimize the hidden layers and selection layers\n",
    "# the selection layer will be optimized on given support only.\n",
    "# the optimzation of hidden layers and selection layer will take turn in iterations\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate, weight_decay=0.0025*c)\n",
    "optimizer0 = torch.optim.Adam(model.hidden0.parameters(), lr=learning_rate, weight_decay=0.0005*c)\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "lf = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Allocated some objects to keep track of changes over iterations\n",
    "hist = []\n",
    "SUPP = []\n",
    "LOSSES = []\n",
    "supp_x = list(range(p)) # initial support\n",
    "SUPP.append(supp_x)\n",
    "\n",
    "\n",
    "### DFS algorithm\n",
    "\n",
    "for i in range(epochs):\n",
    "    # One DFS epoch\n",
    "    \n",
    "    model, supp_x, LOSS = DFS_epoch(model, s, supp_x, X, Y, lf, optimizer0, optimizer, Ts, step)\n",
    "    LOSSES = LOSSES + LOSS\n",
    "    supp_x.sort()\n",
    "    # Save current loss function value and support\n",
    "    hist.append(lf(model(X), Y).data.numpy().tolist())\n",
    "    SUPP.append(supp_x)\n",
    "    # Prevent divergence of optimization over support, save the current best model\n",
    "    if hist[-1] == min(hist):\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        best_supp = supp_x\n",
    "    # Early stop criteria\n",
    "    if len(SUPP[-1]) == len(SUPP[-2]) and (SUPP[-1] == SUPP[-2]).all():\n",
    "        break\n",
    "\n",
    "print(\"Training finished in\" , len(SUPP)-1, \"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk, we will demonstrate the results from the DFS algorithm, in terms of selected support, training error and test error for __one step__ procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The support selected is: [ 3 23]\n",
      "The index of non-zero coefficents on selection layer: [ 3 23]\n",
      "The training error is: 0.0\n",
      "The test error is: 0.0\n"
     ]
    }
   ],
   "source": [
    "### Metric calculation\n",
    "\n",
    "err_train_1 = 1-accuracy(best_model, X, Y)\n",
    "err_test_1 = 1-accuracy(best_model, X_test, Y_test)\n",
    "print(\"The support selected is:\", best_supp)\n",
    "print(\"The index of non-zero coefficents on selection layer:\", \n",
    "      np.where(best_model.hidden0.weight != 0)[0])\n",
    "print(\"The training error is:\", err_train_1)\n",
    "print(\"The test error is:\", err_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we have successfully selected the right support, i.e. the first 4 variables. (Note in `python` starting index is 0)\n",
    "\n",
    "In the following chunk, we will perform a two-step procedure to train the `best_model` on the given support.\n",
    "\n",
    "Two-step procedure is used for two reasons, to get better predictive performance and to get better estimation of $bic$ which is important in selection of optimal $s$.\n",
    "\n",
    "As we demonstrated on the above chunk, the selection layer of `best_model` has non-zero coefficients on given support. In the second step, we treat `best_model` as our initial model and update parameters only in hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy of two step is: 100.0%\n",
      "The test accuracy of two step is: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer only update parameters in hidden layer.\n",
    "_optimizer = torch.optim.Adam(list(best_model.parameters())[1:], lr=0.01, weight_decay=0.0025)\n",
    "# Training\n",
    "for _ in range(100):\n",
    "    out = best_model(X)\n",
    "    loss = lf(out, Y)\n",
    "    _optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    _optimizer.step()\n",
    "\n",
    "### metric calculation\n",
    "acc_train = accuracy(best_model, X, Y)\n",
    "acc_test = accuracy(best_model, X_test, Y_test)\n",
    "print(\"The training accuracy of two step is: \", acc_train*100, \"%\", sep=\"\")\n",
    "print(\"The test accuracy of two step is: \", acc_test*100, \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has shown that the predictive performance of our model is increased. \n",
    "\n",
    "All good results shown above is based on the correct given $s$. However, in reality, $s$ is unknown for most of the time. So the next thing would be finding the optimal $s$\n",
    "\n",
    "### Selection of $s$\n",
    "In this section, we demonstrate the procedure of selection of optimal $s$. We have wrapped up the training procedure above in a function `training_n`. For each given $s$, $bic$, defined as $-2 \\cdot \\log \\hat{L} + s \\cdot \\log n$, of the model will be automatically calculated by `training_n`, also the trained model with the given $s$ will also be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ss = list(range(2, 50)) # We shorten the candidates list in the notebooks\n",
    "BIC = [] # Store the bic for different s\n",
    "best_model = Net_nonlinear(n_feature=p, n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_output=2)\n",
    "for i, s in enumerate(Ss):\n",
    "    # Training dataset k with given s\n",
    "    model, supp, bic, _, [err_train, err_test] = training_n(X, Y, X_test, Y_test, c, s, \n",
    "                                                            epochs=10, Ts=25)\n",
    "    # Store bic values\n",
    "    BIC.append(bic)\n",
    "    # if current bic is the smallest, save the trained model, support and other metric\n",
    "    if bic == min(BIC):\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        best_supp = supp\n",
    "        best_err_train, best_err_test = err_train, err_test # one step model training and testing error\n",
    "\n",
    "idx = np.argmin(BIC)\n",
    "best_s = Ss[idx]\n",
    "plt.plot(Ss, BIC)\n",
    "plt.axvline(x=best_s, ls='--', label=\"optimal s\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can tell $s=4$ is the optimal $s$, and the corresponding model is stored in `best_model` which is the same model showed in section 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
